\chapter{Classifying SLSN using Machine Learning}
\label{Chapter5}
\lhead{Chapter 5. \emph{Machine Learning Classification}}

Throughtout this thesis, I worked with an underlying theme of performing SN classifications, focusing particularly on the selection of SLSN. In \cref{Chapter3}, I establised a definition of SLSNe in terms of the parameter space of the spin-down of a Magnetar model, later used to photometrically classify one SLSNe in the SNLS archival data. In this chapter, I start by describing these techniques as applied to the DES data set during the real-time search for SN in seasons two and three. This included both the manual scanning of SNe and combining it with the Magnetar model fitting. While successful in identifying several, later confirmed, candidate SLSNe, a number of misclassifications highlighted a need for a more robust approach.

In recent years the whole field of astronomy entered a new data analysis renaissance, utilising the Big Data tools and Machine Learning techniques to extract more from archival data and prepare for the arrival of new surveys such as Gaia, LSST, SKA that are expected to produce stagering amounts of data. These are not only difficult to handle for astronomers used to working on much smaller data samples but will infact require absolute state-of-the-art facilies and tools to handle and analyse the data streams. Following this revolution, I endevored to apply some of the latest techniques in ML to the problem of SN classification.

To date, there has been a number of SN studies aiming at classifying SN with the help of ML. Here, I am focusing only on the studies classifying the light curves of SNe and not point source classification pipelines such the once used in DES \citep{Goldstein2015} and Suburu \citep{Morii2016}. Amongst a number of similar works \citep{Karpenka2012,Moller2016,Charnock2016} the most thorough and indepth study of ML classification of SNe is presented in \citet{Lochner2016}. In their work, a number of models are used to extract a range of light curve features. They also provide a comparison of a number of Supervised ML algorithms and discuss their merits in terms of SN classification. While thorough in their analysis, their approach is not ready for deployment in a real survey. The training sampled used in their analysis is the SPCC dataset containing a sanitised sample of SNe only. In a survey such as DES we would first have to separate SNe from other types of transients which, when considering ML techniques, can be performed at the same stage as the classification of SNe.

The use of the SPCC dataset, containg only 2000 SNe weighted to match their observed rates, by \citet{Lochner2016} along with all previous ML classification studies of SNe is one of their greatest drawbacks. In this thesis, I aim to provide the first SN classification study utilising a large (tens of thousands objects of each class), artificially generated sample of objects. With the size of the training sample I tackle the common issue with overfitting for the less common subclasses of SNe. Furthermore, by placing the SNe uniformally throughtout the DES observing season and at a wide range of redshifts, I introduce real survey inperfections including objects whoch suffer from season edge effects and low S/N.

One of the greatest differences between all previous studies and this thesis is the family of ML algorithms used. \citet{Lochner2016} used the SALT2 model, in the process commonly refered to in ML as Feature Extraction or Feature Engineering, to extract a set of parameters that describe the light curve. These are then fed into the machine learning algorithm to produce their classifications. In this thesis, I use Convolutional Neural Networks (CNN), an overwelmingly powerful technique which dominates in the world of commercial ML solutions. CNNs, described in detail in \sref{sec:CNN}, use the data directly as they building their feature sets as part of the learning process. The use of a such a complex ML tool is only possible thanks to the size of the training sample and the data augmentation described in \sref{sec:DataAugmentation}.

In this chapter, I describe the process of creating an artificial training sample of SNe for a majority of subclasses, based on the tools developed in \cref{Chapter4}, as well as AGNs and noise spikes which form the majority of transients detected by DES. I then describe the steps taken to apply the survey noise model to the otherwise smooth simulated data before interpolating and ugmenting it with the help of GPR, described in \sref{sec:GP}. Finally, I discuss the use of the CNN framework to provide a photometric classification for all transients detected by DES in the first four years of its operations.

\section{Search for SLSN in DES using a Non-ML}
Before using machine learning to find SLSN I have used the magnetar model in the same way as in \cref{Chapter2}. This did produce some results and resulted in the classification of a few SLSNe in the data, however, we then started finding objects which really did not fit the model at all and sometimes even had a Chi2 of 3000, as in the case of DES15S2nr. Because of this, we knew that we need to try a different approach.

\subsection{Manual scanning of Transients}
text
\subsection{Magnetar Model Fitting}
text

\section{Training sample}
Machine Learning is the modern approach to problems of classification and broadly data analysis. It has been used throughout SN science for a while now mainly in surveys as a tool for distinguishing between real detections and artifacts in astronomical images. This has been very successful [CITE PTF, DES etc].

\subsection{SN Classification using ML}
In recent years attempts have been made to use ML techniques for photometrically classifying SN from their light curves. The current-state-of the-art work was done by \citet{Lochner2016} which attempted a classification of SN-Ia vs CCSN as well as discriminating between different CCSN subtypes. The project did, however, suffer from a small training sample used in the analysis, with only several hundred SN-Ia and as few as 3 SN-Ib! The training sample used was the Supernova Photometric Classification Challenge (SPCC) \citep{Kessler2010} which was used to develop techniques of photometric classification of SN-Ia for the purpose of the DES cosmology project. The results were not optimal and left a lot of room for improvement inspiring a number of groups to try to improve on this work including this thesis.

Another challenge that has not been tackled by other publication in the field is the classification of SN types not simply amongst a perfect sample of confirmed SN (Like in the case of using the SPCC dataset) but using a realistic, survey-like sample, that contains contamination from noise that spuriously passes the real transient condition as well as AGNs and other variable objects missed in the veto catalogs. In order to achieve this, we need to make reasonable assumptions about the types of the contamination in our SN survey data and add these objects to our training samples.

\section{Data Sources for Training Sample}
In this section, I will list all types of transients injected into our machine learning pipeline and the motivation behind that choice as well as the objects that have been considered but have never been used in the analysis.

\subsection{Noise}
Visual inspection of the data shows that, while rare, some objects flagged as a real transient according to the DES transient naming criteria \cref{Chapter2} do not appear to be physical in origin. There are two common sources of this: bad subtractions and coincidental noise. Despite the best efforts of the DES transient detection pipeline, some objects (often elongated with negative flux [CITE AND INSERT AN IMAGE]) often pass machine learning with a low score. In some cases (weather dependent or otherwise) this may happen in few consecutive nights leading to a real "transient" flag. Alternatively, a slow-moving object may be detected at the same position in 2 (or 3) consecutive filters at the same position. Usually, this will mean detections in \textit{gri} as \textit{z}-band always has a longer exposure. By itself, this would not result in a "transient" classifications but in combination with bad subtractions of random noise spikes exceeding the 5 sigma detection within 30 days cut can result in a "transient" flag.

To model these objects we can use a very simple approach of inserting a number of sharp spikes in the data, correlated between the filters. This account for the moving objects well. Through testing, we found that to account for bad subtractions the easiest approach is to insert several spikes, with high signal to noise that is correlated both temporally and in flux space.

\subsection{AGN}
AGN form by far the largest contamination to the survey data. This is partially due to their physical behaviour and partially due to the caveats of the survey design and planning. AGNs often can have a long period of variability leading to the rise and decline in their light curves on scales comparable to SN. In the cases of SLSN, this is particularly troubling as SLSN can be the same duration as a DES observing season making it very possible for an AGN to act as an SLSN impostor if one considers only one season of DES data and not the remainder of the data. Looking at the multi-season light curves it is often very straightforward to identify AGNs though their multi-season variability. A caveat of the survey is that negative "detections" are not considered as detections. This means that if the object was brighter in the SV1 data set, which was used for creating the first set of DES templates than it has been at any point in the first season it would not have been detected until a much later season. Because of this, AGNs have still been misidentified as new transients as late as the last season of DES.

Due to the stochastic nature of AGN, it is now possible to model them in a repetitive way. It is, however, possible to estimate the behaviour of an AGN using a stochastic, random walk model. In this approach, only the basic properties are required such as the mass of the supermassive black hole and the size and orientation of the accretion disk. We use the CODE\_NAME [CITE SEB] to generate 100000 AGN over a period of 10 years of their variability in the DES filters with a daily cadence.

\subsection{SN-Ia}
SN-Ia is perhaps the most well-understood class of objects included in our training sample. Thanks to their use in observational cosmology and their standardizability, there exists a number of codes capable of modelling these objects at a very high precision. We also understand the parameter space and distribution of the SN therein for the 'normal', cosmologically useful SN [CITE] as well as their more peculiar subclasses.

We use SNANA \citep{Kessler2010} to generate the light curves within DES. Thanks to the fact that SNANA is designed to generate light curves for the cosmological simulations it is set up and has been tested thoroughly to simulate SN-Ia in the most representative way. We set the upper redshift cut to be z=1.4 which we know, from previous work on SN-Ia simulations a redshift at which no objects can be detected by DES. We also use the rate evolution from \citet{Perrett2012} as the most precise and high redshift measurement.

The advantage of SNANA is that it simulates not just the supernovae as perfect events but also applies the observing cadence and conditions to the light curve making them look like real events. This is done using SNANA simulation files (SIMLIB) which record all atmospheric and other observing conditions as measured in real DES images. SNANA can only do this for SN-Ia, we, therefore, duplicate this behaviour using a custom code for other types of objects as described in the [CITE NEXT SECTION]

\subsection{CCSN}
While the rate of CCSN is higher than the rate of SN-Ia in the local universe, making the fraction of CCSN to SN-Ia 70\%-30\%, CCSN are a lot fainter objects with an average brightness a tenth of that of SN-Ia. This means that in a survey like DES we can only detect them to the redshift of z$\sim$0.5 instead of the z=1.0 for SN-Ia. The diversity of CCSN is perhaps the greatest amongst our entire training sample ranging between fast, stripped envelope events that can be serious source of contamination for the samples of SN-Ia because of the similarity in their light curves, driven by the powering of Nickel decay in both cases, to long, plotting SN-II which can have a great diversity in their light curve shapes due to the variation in the strength of CSM interactions which powers them.

Many previous attempts were made to make templates of CCSN and use them in packages such as SNANA [CITE EVERYONE]. While from the technical standpoint these were all very good attempts, the biggest drawback suffered by these templates was the scarcity of the data available to make them. Especially in the cases such as SN-Ib, the templates often contain no more than 3 objects. It was therefore pivotal that we expand these templates to the maximum number of objects available in the literature and work on artificially augment the dataset to match the sample size required by Neural Networks.

\subsubsection{Stripped Envelope SN}
Stripped Envelope SN, namely SN-Ib/c and SN-IIb are a very hot topic in the subject of modelling SN for cosmological purposes. Because their powering mechanism is very similar to that of SN-Ia (e.g. though the decay Ni56 and reprocessing of that radiation in the outter envelope of the ejecta), they often appear extremely similar in light curves causing a potentially large contamination in the sample of photometric SN-Ia that surveys such as DES and LSST may want to use to perform cosmological fitting.

In [CITE CHAPTER 4] I described our approach to creating templates of SN using the CoCo package. While originally written for the PLAsTiCC projects, the code could be trivially extended to DES to produce a large simulation of CCSN. We simulate the CCSN up to z=1.0 as the wide range of possible luminocities does not limit these events from being detected at higher redshift despite the majority of the population expected at z$\sim$0.3. We also assume that the rate of CCSN follows the star formation rate evolution of the universe and anchor the result to the local value found by the SNLS [CITE BAZIN]. \\

\subsubsection{Hydrogen-rich SN}
Hydrogen-rich SN, or SN-II, are more difficult to define as a class of events than any other type. Their diversity in luminocity can reach over 5 magnitudes and their shapes can vary from SN-Ic like events in case of narrow emission line SN-IIn to long and plateoing SN-IIP events. Nonetheless we must attempt to replicate, to the best of our abilities, the behaviour of these objects for the purpose of our simulations.

We modify the CoCo code [CITE CHAPTER 4] to include XXXXX model [CITE] and found the following templates XXXX [INSERT EVERYTHING THAT BERTIE HAS DONE AND EVERYTHING ELSE THAT I HAVE DONE HERE. IF WE END UP DOING TOO MUCH WORK THIS WILL HAVE TO BECOME ITS OWN SECTION, BUT I DOUBT IT]

\subsection{SLSN}
[CITE : I AM LEAVING THIS SECTION UNTIL I HAVE A MODEL GOOD ENOUGH TO TALK ABOUT, OTHER THAN TOYAH]

\subsection{Others}
There are many types of possible transient objects which we have not accounted for here. In some cases we did this because it is unlikely that we would ever detect any (GW counterpart) or because our understanding of them is so limited that we could not provide a comprehensive model for them that could capture the class. This is mostly the case with fast transients [CITE MIIKA] which can appear as having virtually any luminocity and colour and the only connection we have between all of them is their rapid evolution. Believed to be the result of a shock breakout passing though a dense and extended envelope of the star these events are very likely to be connected to the "bumps" often found in SLSN events. We can try to model them as such and see if we can recover such objects [CITE : I REALLY NEED TO TEST THIS, I WOULD HEAVILY BE STEPPING ON MIIKA'S TOES BUT WHO CARES]

\section{Data Augmentation} \label{sec:DataAugmentation}
text
\subsection{Choosing the observing block}
text
\subsection{Choosing the cadence}
text
\subsection{Applying Flux correction to Real Data}
text
\subsection{Applying GPs}
text

\section{Classifications}
text
\subsection{Convolutional Neural Networks}
text
\subsection{Feeding the data}
text
\subsection{SNe vs AGN vs Noise}
text
\subsection{Classifying SNe}
text
\subsection{SLSNe in DES}
